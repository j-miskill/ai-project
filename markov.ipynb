{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Warning \n",
    "\n",
    "This project involves the use of lots of inappropriate and downright offensive/upsetting language. Some of the data analysis methods below print these texts to the notebook so they are visible to whoever opens it. Those who view the notebook please do so knowing this in advance. \n",
    "\n",
    "Topics: sexism, racism, misogyny, and lots more unfortunately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "\n",
    "Here I will be using hidden markov models to detect hate speech "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code : \n",
    "\n",
    "First we will import the necesary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hunte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hunte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hunte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now read in the data file, printing the size and the first five data entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size =  1996\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>isHate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You should know women's sports are a joke</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You look like Sloth with deeper Down’s syndrome</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You look like Russian and speak like Indian. B...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women deserve to be abused, I guess.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Women are made for making babies and cooking d...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  isHate\n",
       "0          You should know women's sports are a joke     1.0\n",
       "1    You look like Sloth with deeper Down’s syndrome     1.0\n",
       "2  You look like Russian and speak like Indian. B...     1.0\n",
       "3               Women deserve to be abused, I guess.     1.0\n",
       "4  Women are made for making babies and cooking d...     1.0"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in input file and output example of first five data entires\n",
    "df = pd.read_csv(\"data/ethos/Ethos_Dataset_Binary.csv\", sep=';', header=0)\n",
    "print(\"size = \", df.size)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will clean and prepare the data to be analyzed, and again check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>isHate</th>\n",
       "      <th>words</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you should know women's sports are a joke</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[you, should, know, womens, sports, are, a, joke]</td>\n",
       "      <td>know woman sport joke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you look like sloth with deeper down’s syndrome</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[you, look, like, sloth, with, deeper, downs, ...</td>\n",
       "      <td>look like sloth deeper down syndrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you look like russian and speak like indian. b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[you, look, like, russian, and, speak, like, i...</td>\n",
       "      <td>look like russian speak like indian disgusting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>women deserve to be abused, i guess.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[women, deserve, to, be, abused, i, guess]</td>\n",
       "      <td>woman deserve abused guess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>women are made for making babies and cooking d...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[women, are, made, for, making, babies, and, c...</td>\n",
       "      <td>woman made making baby cooking dinner nothing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  isHate  \\\n",
       "0          you should know women's sports are a joke     1.0   \n",
       "1    you look like sloth with deeper down’s syndrome     1.0   \n",
       "2  you look like russian and speak like indian. b...     1.0   \n",
       "3               women deserve to be abused, i guess.     1.0   \n",
       "4  women are made for making babies and cooking d...     1.0   \n",
       "\n",
       "                                               words  \\\n",
       "0  [you, should, know, womens, sports, are, a, joke]   \n",
       "1  [you, look, like, sloth, with, deeper, downs, ...   \n",
       "2  [you, look, like, russian, and, speak, like, i...   \n",
       "3         [women, deserve, to, be, abused, i, guess]   \n",
       "4  [women, are, made, for, making, babies, and, c...   \n",
       "\n",
       "                                           processed  \n",
       "0                              know woman sport joke  \n",
       "1               look like sloth deeper down syndrome  \n",
       "2  look like russian speak like indian disgusting...  \n",
       "3                         woman deserve abused guess  \n",
       "4  woman made making baby cooking dinner nothing ...  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data to lowercase, remove all special characters, and tokenize\n",
    "df['comment'] = df['comment'].str.lower()\n",
    "df['words'] = df['comment'].str.replace(r'[^ \\w\\s]', '', regex=True)\n",
    "df['words'] = df['words'].apply(nltk.word_tokenize)\n",
    "\n",
    "# defining stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# defining clean and process\n",
    "def clean_and_process(row) :\n",
    "    processed_words = []\n",
    "    for word in row['words'] :\n",
    "        if word not in stop_words :\n",
    "            lemmatized = lemmatizer.lemmatize(word)\n",
    "            processed_words.append(lemmatized)\n",
    "    return (' '.join(processed_words))\n",
    "\n",
    "df['processed'] = df.apply(clean_and_process, axis=1)\n",
    "\n",
    "# print first five to check cleaning\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.sample(frac = 0.25, random_state = 1)\n",
    "test = df.drop(train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will start by seperating the training data into hate / not hate. In the below I use bigrams to create probability maps for sequential words in both training hate classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization of the \"train\" training data\n",
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "X_train = vect.fit_transform(train['processed'])\n",
    "Y_train = train['isHate'] # unused\n",
    "\n",
    "# defining a barrier for classifying test data as hate or not hate\n",
    "barrier = 0.16666667\n",
    "# seperating training data based on isHate\n",
    "hate_yes = train[train['isHate'] > barrier]\n",
    "hate_no = train[train['isHate'] <= barrier]\n",
    "\n",
    "# transform both sets into bigram frequency vectors\n",
    "X_train_hate_yes = vect.transform(hate_yes['processed'])\n",
    "X_train_hate_no = vect.transform(hate_no['processed'])\n",
    "\n",
    "# calculate the frequency of each bigram across all comments in each class\n",
    "freq_hate_yes = np.array(X_train_hate_yes.sum(axis=0)).flatten()\n",
    "freq_hate_no = np.array(X_train_hate_no.sum(axis=0)).flatten()\n",
    "# total count of bigrams in each class\n",
    "total_hate_yes = freq_hate_yes.sum()\n",
    "total_hate_no = freq_hate_no.sum()\n",
    "\n",
    "# proportions of hate to not hate in the training set\n",
    "prop_hate_yes = len(hate_yes) / len(train)\n",
    "prop_hate_no = len(hate_no) / len(train)\n",
    "# probabilities of bigrams in each class\n",
    "prob_bigram_hate_yes = freq_hate_yes / total_hate_yes\n",
    "prob_bigram_hate_no = freq_hate_no / total_hate_no\n",
    "\n",
    "# extract bigram feature names\n",
    "feature_names = vect.get_feature_names_out()\n",
    "# creating new dataframes for each class that map the bigrams to their probabilities\n",
    "# within that class \n",
    "hate_yes_df = pd.DataFrame({'bigram': feature_names, 'probability': prob_bigram_hate_yes})\n",
    "hate_no_df = pd.DataFrame({'bigram': feature_names, 'probability': prob_bigram_hate_no})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a method to predict whether a new sentence is hate speech given the above bigram dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hate_speech(sentence) :\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    bigrams = [' '.join(bigram) for bigram in zip(tokens[:-1], tokens[1:])]\n",
    "    log_prob_yes = np.log(prop_hate_yes)\n",
    "    log_prob_no = np.log(prop_hate_no)\n",
    "    \n",
    "    # silence divide by zero warnings cause have laplace smoothing\n",
    "    np.seterr(divide='ignore')\n",
    "\n",
    "    # Calculate the probability of each bigram\n",
    "    for bigram in bigrams :\n",
    "        if bigram in hate_yes_df['bigram'].values :\n",
    "            log_prob_yes += np.log(hate_yes_df.loc[hate_yes_df['bigram'] == bigram, 'probability'].values[0])\n",
    "        else:\n",
    "            log_prob_yes += np.log(1e-6)  # laplace smoothing for unseen bigrams\n",
    "            \n",
    "        if bigram in hate_no_df['bigram'].values :\n",
    "            log_prob_no += np.log(hate_no_df.loc[hate_no_df['bigram'] == bigram, 'probability'].values[0])\n",
    "        else :\n",
    "            log_prob_no += np.log(1e-6)  # laplace smoothing for unseen bigrams\n",
    "            \n",
    "    return 1 if log_prob_yes > log_prob_no else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, definining a method to test the accuracy of the above model on a fresh dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.34\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy(dataframe) :\n",
    "    correct_predictions = 0\n",
    "    total_rows = len(dataframe)\n",
    "\n",
    "    for index, row in dataframe.iterrows() :\n",
    "        prediction = predict_hate_speech(row['processed'])\n",
    "        actual = row['isHate']\n",
    "\n",
    "        if prediction == actual :\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    if correct_predictions == 0 :\n",
    "        accuracy = 0.0\n",
    "    else :\n",
    "        accuracy = correct_predictions / total_rows\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "accuracy_score = test_accuracy(test)\n",
    "print(f\"Accuracy: {accuracy_score: .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found an accuracy of 34% on detecting hate speech using markov models with bigrams. Changing the barrier of seperation for hate / not hate drastically impacts the resulting bigram model and detection accuracy. Lowering the barrier to accomodate for lighter hate speech increases accuracy, but only at the cost of calling everything hate speech, not actually being selective.\n",
    "\n",
    "34% is obviously awful given that with a barrier of 0.166667, the dataset is balanced and random guessing would result in around 50% accuracy. \n",
    "\n",
    "Will work through improving the model later, this is far as I can get now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
